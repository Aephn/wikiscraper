yield response.follow(next_page, callback=self.parse)
    yield response.follow(url, callback=self.parse_page)?

def is_wikipage(self, path: str) -> bool:
    """Exclude any pages that are not actual wikipedia pages but instead other
    functional pages for Wikipedia, e.g. 'Template:', etc.

    NOTE: This method may disallow for some pages to be shown if they have a colon, 
    however this shouldn't (?) be an issue unless a colon is somehow put into an internal
    page.
    """
    # disallowed_pages = ["Wikipedia:", "Template:", "Special:", "Talk:", "Help:", "File:"]

    # using a broad if ':', since the disallowed pages have colons.
    if ':' in path:
            return False
    return True

def rm_duplicates(self, urls: list) -> list:
    """Remove Duplicates in list of URLs
    
    NOTE: This does cause a significant? increase in runtime.
    """
    no_dup_list = []

    for url in urls:
        match = re.search(r"(?P<url>https?://[^\s]+)", url)  # I have no clue what this does.
        if match is not None:
            no_dup_list.append((match.group("url")))
    return no_dup_list


    
    # def export_all():
    #     """Export all parent pairs to a JSON file"""
    #     # https://docs.scrapy.org/en/2.11/topics/exporters.html#scrapy.exporters.JsonItemExporter
    #     scrapy.exporters.JsonItemExporter()
    #     pass
